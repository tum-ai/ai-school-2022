{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![grafik](https://github.com/tum-ai/ai-school-2022/blob/main/images/tum-ai-logo.png?raw=true)"
      ],
      "metadata": {
        "id": "yk5WjtQecjMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "Our goal in this notebook is to create a simple chatbot.\n",
        "\n",
        "We will do this by using an already trained model from hugging face and creating a small interface to be able to write with the model. Once this is done, you can yourself do a small \"Turing test\" and see how well the model is able to communicate like a human."
      ],
      "metadata": {
        "id": "Xn3n4-PSNwOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Preparing Hugging Face ðŸ¤—"
      ],
      "metadata": {
        "id": "3AkynuwVbvYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T97SF4QCaQi2",
        "outputId": "de4e7361-f02b-45fb-b11c-7ac9cfeff9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n"
          ]
        }
      ],
      "source": [
        "# Installing Hugging Face\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Selecting the model\n",
        "To create a chatbot, we need a model which is able to take in our previous text messages and then uses this information to create a new text response. Therefore we are using a Text Generation model."
      ],
      "metadata": {
        "id": "toYGFGRVO-t_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to use the already trained model gpt2, which stands for generative Pre-trained Transformer."
      ],
      "metadata": {
        "id": "gDf4oBgiR5g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import huggingface\n",
        "from transformers import pipeline, GPT2Tokenizer\n",
        "\n",
        "# load GPT-2 model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Load tokenizer for GPT-2 model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "0oA596jiUQpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating some examples\n",
        "# Feel free to add any prompt here and test the capabilities of the model\n",
        "prompt = \"Once upon a time,\"\n",
        "\n",
        "# generating sequences and printing them\n",
        "sequences = generator(prompt, max_new_tokens=20, num_return_sequences=3, eos_token_id=tokenizer.eos_token_id)\n",
        "for i, sequence in enumerate(sequences):\n",
        "  print(f'Completion {i+1}:')\n",
        "  print(sequence['generated_text'], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-ig2cvxeo16",
        "outputId": "9c2eb75d-4570-417b-d147-024d75b5cf52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion 1:\n",
            "Once upon a time, the entire cosmos was made up of millions of tiny galaxies, the vast majority of which are not yet \n",
            "\n",
            "Completion 2:\n",
            "Once upon a time, you see her staring at you in disbelief. How did you feel about this? If you said it \n",
            "\n",
            "Completion 3:\n",
            "Once upon a time, I was living in the shadow of my old self before I became completely immersed in the reality the world \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Designing the prompt\n",
        "As of now, we can use the model to continue writing a given prompt. But for our chatbot, we do not simply have a long text but instead multiple messages. Thats when we enter the territory of `Prompt Engineering`.\n",
        "\n",
        "Our first task is to represent multiple messages in a single text prompt. Here we chose to do that by restricting messages to only be one line and writing the auther of the message in each line."
      ],
      "metadata": {
        "id": "l-JGjtdPU9iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to edit the prompt and replace the messages or add new ones\n",
        "prompt = \"\"\"Human: Hello, how are you?\n",
        "AI: I am fine. Thanks.\n",
        "Human: Do you like pineapple on pizza?\n",
        "AI: No.\n",
        "Human: Good, what did you do yesterday?\n",
        "AI:\"\"\"\n",
        "\n",
        "# generating sequences and printing them\n",
        "sequences = generator(prompt, max_new_tokens=20, num_return_sequences=3, eos_token_id=tokenizer.eos_token_id)\n",
        "for i, sequence in enumerate(sequences):\n",
        "  print(f'Completion {i+1}:')\n",
        "  print(sequence['generated_text'], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQU7JJemOhZt",
        "outputId": "5d091152-3825-4631-b764-1db0d9bc1e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion 1:\n",
            "Human: Hello, how are you?\n",
            "AI: I am fine. Thanks.\n",
            "Human: Do you like pineapple on pizza?\n",
            "AI: No.\n",
            "Human: Good, what did you do yesterday?\n",
            "AI: Nothing. I just got into a ruckus with my roommate.\n",
            "Crazy Planet: He \n",
            "\n",
            "Completion 2:\n",
            "Human: Hello, how are you?\n",
            "AI: I am fine. Thanks.\n",
            "Human: Do you like pineapple on pizza?\n",
            "AI: No.\n",
            "Human: Good, what did you do yesterday?\n",
            "AI: I cooked at 1:05.\n",
            "Human: Oh, sorry. What are you doing?\n",
            " \n",
            "\n",
            "Completion 3:\n",
            "Human: Hello, how are you?\n",
            "AI: I am fine. Thanks.\n",
            "Human: Do you like pineapple on pizza?\n",
            "AI: No.\n",
            "Human: Good, what did you do yesterday?\n",
            "AI: Wellâ€¦ I got my hair done after getting an appointment at the pharmacy.\n",
            "Humans: How \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model tries to continue the conversation, we can still do some more prompt engineering. We can, for example, give the bot a more character/personality by adjusting the prompt a bit."
      ],
      "metadata": {
        "id": "kyMKCJqcdXIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding emojis or changing the answers can have quite an impact on\n",
        "# the responses \"personality\"\n",
        "prompt = \"\"\"Human: Hello, how are you?\n",
        "AI: Heeey, I am fine. Thanks :)\n",
        "Human: Do you like pineapple on pizza?\n",
        "AI: Haha, no, you better keep that far away from me! :P\n",
        "Human: Good, what did you do yesterday?\n",
        "AI:\"\"\"\n",
        "\n",
        "# generating sequences and printing them\n",
        "sequences = generator(prompt, max_new_tokens=20, num_return_sequences=3, eos_token_id=tokenizer.eos_token_id)\n",
        "for i, sequence in enumerate(sequences):\n",
        "  print(f'Completion {i+1}:')\n",
        "  print(sequence['generated_text'], '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuTc_kKNazyn",
        "outputId": "aff606e6-99c1-4627-be6c-52589ccec70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion 1:\n",
            "Human: Hello, how are you?\n",
            "AI: Heeey, I am fine. Thanks :)\n",
            "Human: Do you like pineapple on pizza?\n",
            "AI: Haha, no, you better keep that far away from me! :P\n",
            "Human: Good, what did you do yesterday?\n",
            "AI: My wife fell asleep.\n",
            "E-H-H-H-H\n",
            "E-H- \n",
            "\n",
            "Completion 2:\n",
            "Human: Hello, how are you?\n",
            "AI: Heeey, I am fine. Thanks :)\n",
            "Human: Do you like pineapple on pizza?\n",
            "AI: Haha, no, you better keep that far away from me! :P\n",
            "Human: Good, what did you do yesterday?\n",
            "AI: Well, I'm just trying to stay awake, so I don't need your sleep all day! \n",
            "\n",
            "Completion 3:\n",
            "Human: Hello, how are you?\n",
            "AI: Heeey, I am fine. Thanks :)\n",
            "Human: Do you like pineapple on pizza?\n",
            "AI: Haha, no, you better keep that far away from me! :P\n",
            "Human: Good, what did you do yesterday?\n",
            "AI: Hmm, a quick stroll around in the village.\n",
            "Human: Where did your parents come from? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Creating an interactive chatbot\n",
        "Now we want to use the output to actually create our chatbot. First, we can reduce"
      ],
      "metadata": {
        "id": "34gI_7rwnaZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Tom: Hi Julian, how are you?\n",
        "Julian: Hi Tom, I am fine. Thanks.\n",
        "Tom: I just wanted to ask you a few things. Do you have some time?\n",
        "Julian: Yea sure. Just ask :D\"\"\"\n",
        "\n",
        "print('Type \"exit\" to exit the chat\\n')\n",
        "for i in range(100):\n",
        "  inpt = input('You: ')\n",
        "  if inpt == 'exit':\n",
        "    break\n",
        "  prompt += '\\nTom: ' + inpt + '\\nJulian:'\n",
        "  sequence = generator(\n",
        "      prompt,\n",
        "      max_new_tokens=50,\n",
        "      num_return_sequences=1,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      no_repeat_ngram_size=3,\n",
        "      do_sample=True,\n",
        "      top_k=50, \n",
        "      top_p=0.7,\n",
        "      temperature = 0.8\n",
        "      )[0]\n",
        "  answer = sequence['generated_text'][len(prompt):]\n",
        "  answer = answer.split('\\n')[0]\n",
        "  print(f'AI: {answer}')\n",
        "  prompt += answer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJJxPEVhlMIE",
        "outputId": "f4e557d0-352a-4a9e-aa67-81d1d1a4f273"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type \"exit\" to exit the chat\n",
            "\n",
            "You: hi\n",
            "AI:  You have some work to do and I am just wondering if you could help me with that.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BjHgb-HjlO-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}